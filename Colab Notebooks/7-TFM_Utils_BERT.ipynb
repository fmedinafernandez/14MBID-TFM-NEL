{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesado de Texto"
      ],
      "metadata": {
        "id": "x017f-XHiSRb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRj-GbKDiNAg",
        "outputId": "fd720af6-d782-4501-e9b0-a58da7527cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-08 17:04:55.197337: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'es' are deprecated. Please use the\n",
            "full pipeline package name 'es_core_news_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from es-core-news-sm==3.4.0) (3.4.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.25.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.0.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "! pip install spacy\n",
        "! python -m spacy download es\n",
        "! pip install transformers\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def NLP_preprocesaDocumento(doc):\n",
        "  #Cargamos el modelo en español\n",
        "  nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "  #Tokenizado\n",
        "  doc = [\" \".join([token.text for token in nlp(doc)])][0]\n",
        "\n",
        "  #Eliminación de signos de puntuación\n",
        "  doc = [\" \".join([token.lemma_ for token in nlp(doc) if not token.is_punct]) ][0]\n",
        "\n",
        "  #Eliminación de stopwords\n",
        "  spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS\n",
        "  doc = [\" \".join([token for token in doc.split() if not token.lower() in spacy_stopwords])][0]\n",
        "  \n",
        "  #Lematización de palabras\n",
        "  doc = [\" \".join([token.lemma_ for token in nlp(doc)])][0]\n",
        "\n",
        "  #Eliminación de tildes\n",
        "  tildes = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u', 'Á': 'A', 'E': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', 'ü': 'u', 'Ü': 'U'}\n",
        "  for tilde in tildes:\n",
        "\t  if tilde in doc:\n",
        "\t\t  doc = doc.replace(tilde, tildes[tilde])\n",
        "\n",
        "  return doc\n"
      ],
      "metadata": {
        "id": "OWmrre3vii8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"Hay evidencia de hipoatenuación hepática difusa compatible con infiltración grasa . No hay dilatación de los conductos biliares intra o extrahepáticos. El paciente se encuentra en estado post colecistectomía. El bazo es normal. El páncreas es de contorno y características de atenuación normales. No hay evidencia de masa suprarrenal. Hay una hernia supraumbilical de tamaño moderado que contiene grasa. Los riñones son normales en tamaño, forma y configuración. No se identifican cálculos renales ni ureterales. No hay hidrouréter ni hidronefrosis. No hay evidencia de apendicitis. Hay varias asas de intestino delgado llenas de líquido, compatibles con una enteritis leve. No hay engrosamiento de la pared intestinal. No hay evidencia de obstrucción del intestino delgado o grueso. No hay evidencia de ascitis abdominal o linfadenopatía. No hay evidencia de masa vesical intrínseca o extrínseca . No hay ascitis pélvica ni linfadenopatía . El útero y los ovarios no presentan ninguna anomalía. Las imágenes de las bases pulmonares no muestran evidencia de masa pleural o parenquimatosa . No hay derrames pleurales. Hay cicatrices en el lóbulo medio derecho y en la língula, así como en ambas bases pulmonares. Las estructuras óseas están libres de lesiones líticas o blásticas. Se observan cambios degenerativos multinivel en la columna toracolumbar. Se observan calcificaciones dispersas en la aorta y en sus principales ramas, compatibles con la aterosclerosis.\""
      ],
      "metadata": {
        "id": "SeK9vG4Oi2ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NLP_preprocesaDocumento(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "xvA_WYtdjFOz",
        "outputId": "9009e3bb-a300-4996-b305-3a98e76f9a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'evidenciar hipoatenuacion hepatico difusa compatible infiltracion gras dilatacion conducto biliar intra extrahepatico paciente encontrar post colecistectomia bazo normal pancrea contorno caracteristico atenuacion normal evidenciar masa suprarrenal hernia supraumbilical tamaño moderado contener grasa riñon normal tamaño forma configuracion identificar calculo renal ureteral hidroureter hidronefrosis evidenciar apendicitis asa intestino delgado llenar liquido compatible enteriti leve engrosamiento pared intestinal evidenciar obstruccion intestino delgado grueso evidencia asciti abdominal linfadenopatir evidencio masa vesical intrinseco extrinseca asciti pelvico linfadenopatir utero ovario presentar anomalio imagen base pulmonar mostrar evidencio masa pleural parenquimatos derrame pleural cicatriz lobulo derecho lingular base pulmonar estructura oseo libre lesion litico blastico observar cambio degenerativo multinivel columno toracolumbar observar calcificacion disperso aorta principal rama compatible aterosclerosis'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga del modelo de Embedding BERT\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W8vsjnNGlynm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí es importante cargar el modelo BERT entrenado para lenguaje médico!!!"
      ],
      "metadata": {
        "id": "WoEsHg8CmjPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "h_DZ58WJly_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "wjxrmuaB0g4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'abdomen'"
      ],
      "metadata": {
        "id": "61wrPJufmB_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outputs = model(**inputs)\n",
        "#outputs[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4dfQ-h5mGcR",
        "outputId": "3c79dd13-b189-46b6-d369-b87fb8c8692d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def NLP_get_BERT_embedding(model, text):\n",
        "  # Add the special tokens.\n",
        "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "  # Split the sentence into tokens.\n",
        "  tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "  # Map the token strings to their vocabulary indeces.\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "  # Mark each of the tokens as belonging to sentence \"1\".\n",
        "  segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # Run the text through BERT, and collect all of the hidden states produced\n",
        "  # from all 12 layers. \n",
        "  with torch.no_grad():\n",
        "\n",
        "      outputs = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "      # Evaluating the model will return a different number of objects based on \n",
        "      # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
        "      # becase we set `output_hidden_states = True`, the third item will be the \n",
        "      # hidden states from all layers. See the documentation for more details:\n",
        "      # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "      hidden_states = outputs[2]\n",
        "\n",
        "  # `hidden_states` has shape [13 x 1 x 22 x 768]\n",
        "\n",
        "  # `token_vecs` is a tensor with shape [22 x 768]\n",
        "  token_vecs = hidden_states[-2][0]\n",
        "\n",
        "  # Calculate the average of all 22 token vectors.\n",
        "  sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "  return sentence_embedding"
      ],
      "metadata": {
        "id": "DnVngv2tacw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjl8bioGRpGl",
        "outputId": "38fa91f2-209e-47e5-8129-4813029798cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our final sentence embedding vector of shape: torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxyEyo8KmV8C",
        "outputId": "08bf80bd-02dc-4327-fb91-3ddce8366a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-6.0830e-01, -2.7166e-01, -3.4278e-01, -4.8407e-01, -4.7151e-02,\n",
              "         1.9344e-01,  1.8054e-01,  3.8789e-01, -3.2630e-01, -9.7103e-01,\n",
              "        -1.1977e-01,  1.5908e-01,  1.2633e-01, -5.9967e-01, -2.9988e-01,\n",
              "         2.0238e-01, -1.0391e-01,  3.2397e-01,  6.3429e-01,  3.6771e-01,\n",
              "         8.1225e-02, -1.6037e-02, -4.5312e-01, -1.8587e-01,  5.5763e-01,\n",
              "        -5.7271e-02,  2.7353e-01,  5.9289e-02, -6.2920e-01,  3.2208e-01,\n",
              "         8.1125e-02,  2.2946e-01,  1.4167e-01,  1.3945e-01, -4.9444e-01,\n",
              "        -4.2282e-01,  1.2071e-01, -2.3081e-01, -3.4686e-01,  8.3958e-02,\n",
              "        -2.4661e-02, -4.0552e-01,  8.1624e-01, -5.4481e-01,  1.5226e-01,\n",
              "        -2.6216e-01, -7.5637e-01,  1.1092e-01, -3.6664e-01, -1.6258e-01,\n",
              "        -8.0120e-01, -2.1154e-01, -5.0239e-01,  3.8217e-01, -1.2875e-02,\n",
              "         4.4980e-02,  5.1710e-01,  1.8922e-01, -3.4892e-01, -6.6097e-01,\n",
              "         7.0411e-02, -1.5129e-01,  1.4710e-01,  4.5724e-02,  8.0800e-02,\n",
              "         1.3840e-01,  3.3532e-01,  6.1696e-01, -1.3423e-01,  3.4665e-01,\n",
              "        -1.0105e+00, -5.6532e-02,  7.2969e-01,  2.6677e-01, -1.7287e-02,\n",
              "         1.6238e-03, -3.4078e-01,  1.3516e-01, -1.8284e-01, -3.8161e-01,\n",
              "        -1.7017e-02,  4.5725e-01, -1.1875e-01, -5.0391e-02, -7.8864e-02,\n",
              "         4.6788e-01, -2.6708e-01, -5.4416e-01,  9.1267e-02,  3.0672e-01,\n",
              "        -7.1493e-01,  1.1499e-01, -2.0482e-01,  6.5934e-02,  1.7137e-01,\n",
              "         3.6377e-01, -4.1577e-01, -4.3708e-01,  6.0160e-03, -7.7134e-01,\n",
              "        -5.3257e-01, -3.5735e-01,  3.9501e-01, -3.8098e-01, -1.4924e-01,\n",
              "        -2.0323e-02, -4.3265e-01, -2.9556e-01,  5.2737e-01,  1.6080e-01,\n",
              "         1.0213e-01,  3.1756e-01,  2.3124e-01, -2.0590e-01, -2.4940e-01,\n",
              "         1.1560e+00, -1.8903e-01, -2.1156e-01,  3.9775e-01, -9.7409e-02,\n",
              "        -1.1187e-01, -3.3854e-01, -9.3683e-02,  2.1954e-01,  2.7448e-01,\n",
              "         1.9763e-01, -2.0404e-01, -1.4157e-01,  2.5380e-01,  2.2539e-01,\n",
              "         2.7932e-01,  5.1610e-01,  2.8713e-01,  3.2298e-01, -5.4355e-01,\n",
              "        -2.0694e-01, -1.8540e-02,  1.6748e-01,  2.9856e-01,  1.6809e-02,\n",
              "        -4.9856e-01, -2.7312e-01, -1.9629e-01, -3.7069e-01,  4.3357e-01,\n",
              "        -1.3827e-01, -5.0473e-01,  1.1595e-01, -3.6108e-02,  1.3628e-01,\n",
              "         6.5345e-02, -1.4300e-02,  2.2916e-01, -1.7100e-01, -4.7053e-01,\n",
              "        -8.2175e-02,  2.0698e-01, -1.1151e-01,  6.5727e-01,  5.2057e-01,\n",
              "         5.6679e-01,  4.0996e-01,  2.2045e-01,  4.1311e-01, -7.1144e-01,\n",
              "         2.1872e-01,  6.0561e-02,  8.0545e-01,  1.2909e-01, -1.0061e-01,\n",
              "        -5.3802e-01, -1.6468e-01,  9.1157e-01,  3.0402e-01, -2.1248e-01,\n",
              "        -1.0285e-01, -4.9670e-01,  2.4722e-01,  3.1940e-02,  3.6768e-01,\n",
              "        -1.5294e+00,  3.8465e-01, -7.3483e-02, -4.4847e-01, -5.9909e-03,\n",
              "         1.8268e-01,  5.9027e-01, -5.6391e-01, -9.9524e-02,  4.9196e-01,\n",
              "         1.8002e-01,  6.3164e-02, -3.8138e-02,  1.6211e-02, -2.8638e-01,\n",
              "        -2.9607e-01,  2.5788e-01, -2.1748e-01,  3.4522e-01, -6.2728e-02,\n",
              "         4.0273e-01,  4.6881e-01,  4.0598e-01, -4.5326e-02, -2.2692e-01,\n",
              "         1.0671e+00, -2.7336e-01, -4.3232e-01, -2.9281e-02,  4.4585e-02,\n",
              "        -4.5785e-01,  7.0715e-01, -4.1433e-01, -2.5288e-01,  3.6242e-01,\n",
              "        -4.6407e-01,  3.9524e-01,  4.8502e-02, -1.7330e-01,  2.5219e-01,\n",
              "         3.6898e-01, -4.5949e-01,  2.1809e-01,  6.1160e-01, -5.6672e-01,\n",
              "         1.9969e-01,  7.5393e-02, -6.8670e-02, -5.7460e-01, -5.2214e-01,\n",
              "         6.6281e-02, -3.5201e-01, -3.1764e-02,  1.4775e-01, -8.8348e-01,\n",
              "         3.9194e-01, -4.2412e-01, -1.0864e-01, -1.0659e-01,  2.6510e-01,\n",
              "        -4.9657e-01,  2.2492e-01,  6.0192e-01,  2.4922e-01, -2.7926e-01,\n",
              "         1.5651e-01,  8.7145e-01, -3.3663e-01, -4.7515e-02,  1.9641e-01,\n",
              "         2.2350e-01, -6.5127e-02, -3.2346e-02, -5.7694e-01, -3.9015e-01,\n",
              "        -2.6020e-01,  6.6814e-01,  3.6612e-01,  9.3969e-02, -3.0596e-02,\n",
              "         1.0256e-01, -1.9766e-01,  4.1132e-01, -4.6686e-01,  4.8411e-03,\n",
              "         2.2652e-01,  6.7286e-01, -3.0666e-01,  3.3569e-01, -2.7300e-01,\n",
              "        -3.7691e-01, -1.3953e-01, -8.3706e-03,  1.2101e-01,  7.7862e-02,\n",
              "        -2.0311e-01,  1.7129e-01,  1.4426e-01, -1.7688e-01, -5.1242e-02,\n",
              "         2.1700e-01, -2.0868e-02, -3.1691e-01,  7.3404e-02,  6.2097e-01,\n",
              "         4.6038e-02, -7.0127e-01, -8.1329e-01,  3.9246e-01, -3.2317e-01,\n",
              "        -3.8436e-02,  2.0274e-01,  1.4171e-01, -6.7440e-02,  4.3463e-01,\n",
              "         1.8521e-01, -3.3709e-01,  9.0281e-02, -2.8541e-01, -4.0293e-01,\n",
              "        -3.4953e-01,  1.9119e-01,  9.4572e-02, -5.9352e-01,  1.6978e-01,\n",
              "         3.2252e-01, -6.6091e-02, -3.6303e-01, -6.2124e+00,  1.5564e-01,\n",
              "        -2.3639e-02, -4.2811e-01,  1.6949e-01, -1.9885e-01, -2.4002e-01,\n",
              "         9.7958e-02, -2.8274e-01, -5.8973e-01,  5.5136e-01, -1.3930e-01,\n",
              "         1.2336e-01, -2.1083e-01,  9.4337e-03, -4.7489e-02, -2.5440e-01,\n",
              "         3.3369e-01, -1.0848e-01,  2.0755e-01, -3.2843e-01,  1.9725e-01,\n",
              "         2.4349e-01,  6.5452e-02, -2.7799e-01, -4.3408e-01, -1.5785e-01,\n",
              "        -2.0725e-02, -5.4145e-01, -3.0848e-01, -6.6696e-01, -1.7211e-01,\n",
              "        -2.7094e-01,  2.6523e-01,  1.4596e-02, -1.4662e-01, -5.1623e-02,\n",
              "        -1.3294e-01,  1.5179e+00,  5.6027e-01, -6.5693e-02, -1.0497e-02,\n",
              "        -4.3338e-01,  4.9124e-01,  4.9045e-01, -1.3640e-01,  3.3088e-01,\n",
              "        -1.5043e-02, -6.9231e-03,  6.8648e-02,  1.5666e-01,  7.1813e-02,\n",
              "         4.0150e-01, -5.7636e-01,  2.4198e-01, -1.4357e-01,  1.9074e-01,\n",
              "         2.9684e-01,  3.3073e-01, -7.6188e-01,  7.1621e-01, -6.0357e-01,\n",
              "         3.2967e-02,  7.9441e-01, -5.7938e-01, -7.6235e-02, -6.0555e-01,\n",
              "        -1.2996e-01,  2.9554e-01, -2.3565e-01,  3.3029e-01, -1.9407e-01,\n",
              "         6.7016e-01, -1.0233e+00, -1.9986e-01, -5.2042e-01, -3.2742e-01,\n",
              "        -1.2571e-01,  2.5753e-01, -7.8882e-01,  9.7021e-02, -8.3835e-02,\n",
              "         1.2155e-01,  2.3470e-01,  4.2558e-02,  1.7345e-01,  2.9512e-01,\n",
              "        -4.8081e-01, -3.0367e-01,  4.3076e-01, -4.6947e-01,  8.4413e-02,\n",
              "        -8.8188e-02,  4.2149e-01, -5.0542e-04,  3.9556e-01,  6.5719e-01,\n",
              "        -5.8481e-01, -1.9053e-01,  3.5867e-01, -4.6347e-01, -4.8814e-01,\n",
              "        -7.6486e-01,  2.7555e-01, -1.2690e-02, -1.0541e-01, -6.1620e-01,\n",
              "         6.3680e-02,  2.3978e-01,  1.4808e-02,  6.3284e-01,  1.4368e-01,\n",
              "         2.9588e-01,  1.1908e-01, -4.0111e-01,  1.6011e-01, -1.5443e-01,\n",
              "        -3.0828e-01,  1.7189e-01,  2.1999e-01,  3.4957e-02, -3.3460e-01,\n",
              "        -5.7121e-02, -2.0798e-01,  1.4659e-01,  3.0411e-01,  2.4072e-02,\n",
              "        -6.7463e-01, -6.5849e-01,  8.9475e-03,  2.5096e-01, -2.6010e-01,\n",
              "        -1.9419e-01,  9.6144e-02, -2.9560e-01,  3.7904e-02,  6.1935e-02,\n",
              "         4.6384e-01, -1.4796e-02,  7.2718e-01,  2.7337e-01,  1.5247e-01,\n",
              "        -3.6032e-01, -1.0653e-01,  1.0709e-01,  5.4091e-01, -6.5236e-02,\n",
              "         3.5575e-02,  1.9551e-01,  6.5052e-01,  2.8398e-01, -9.4806e-01,\n",
              "         5.5382e-01,  3.4168e-01,  3.1568e-02, -2.6567e-01,  2.3675e-01,\n",
              "        -3.6306e-02, -3.2410e-01, -3.4442e-01, -1.7722e-02, -3.7375e-01,\n",
              "        -5.8412e-01,  3.6735e-01,  3.7512e-01,  1.4116e-01,  1.0461e-01,\n",
              "        -1.6572e-01, -5.3493e-01,  1.6960e-01,  4.2730e-01, -2.8011e-01,\n",
              "        -4.4137e-02,  3.7174e-01, -3.3356e-02, -4.0574e-01, -1.3621e-01,\n",
              "        -5.2311e-02,  1.7084e-01,  4.0213e-01,  2.1772e-02,  1.3513e-01,\n",
              "        -1.5275e-01, -3.9048e-01,  1.9643e-01, -3.7762e-01, -3.5059e-01,\n",
              "        -1.5630e-01,  8.1220e-02,  4.7608e-01, -1.8187e-01, -9.3883e-01,\n",
              "         3.1465e-01, -2.0990e-01, -2.9150e-03,  1.5999e-02,  1.4457e-01,\n",
              "         6.6292e-01,  3.3893e-01,  4.3044e-01, -5.7052e-02, -9.2128e-01,\n",
              "         2.3032e-01,  1.1764e-01, -4.4098e-01,  5.4194e-01,  5.5876e-01,\n",
              "         1.4619e-01, -3.8287e-01, -3.0134e-01,  5.5537e-01,  1.9843e-01,\n",
              "        -9.7038e-02, -3.3315e-01, -6.4085e-01,  5.3729e-01, -1.2544e-01,\n",
              "         6.0191e-01,  1.8791e-01, -1.3481e-01, -6.4273e-01, -5.9786e-01,\n",
              "         1.4422e-01, -8.3658e-02,  3.1844e-01,  1.5863e-01, -4.1671e-01,\n",
              "        -6.6182e-01,  2.5037e-02, -6.4878e-01, -4.3880e-01,  6.2240e-01,\n",
              "         2.2039e-01,  3.2767e-01,  4.4061e-01, -8.2999e-02, -4.1790e-01,\n",
              "        -3.3514e-02, -1.2229e-02, -2.7936e-01,  5.1765e-02, -1.0930e+00,\n",
              "        -1.2244e-01,  1.0586e-01, -5.1075e-01,  3.2743e-01, -5.5954e-01,\n",
              "        -1.3614e-01,  3.7512e-01,  8.9863e-01,  1.6378e-01, -5.5576e-01,\n",
              "        -6.0793e-01, -6.1252e-02,  4.4032e-02, -7.2117e-01, -1.2941e-01,\n",
              "        -1.4677e-02,  5.5581e-02,  4.3991e-01, -3.3350e-01,  7.3714e-01,\n",
              "        -3.5794e-02,  5.1123e-01, -1.7351e-01, -8.7066e-02, -2.3389e-01,\n",
              "         9.9136e-01,  1.8706e-01,  2.0700e-01, -2.7050e-02,  1.7259e-01,\n",
              "        -1.0653e-01, -1.8900e-01, -1.8660e-01, -2.8506e-02,  3.9433e-01,\n",
              "         3.5070e-01, -3.6356e-01,  3.6295e-01, -2.9517e-01,  3.2229e-01,\n",
              "         5.9065e-01, -1.2110e-02,  6.3269e-01, -1.7682e-01, -1.9206e-01,\n",
              "        -1.7208e-01,  1.4646e-01, -5.2698e-01, -2.0911e-01, -2.2519e-01,\n",
              "        -2.9322e-01,  1.7642e-01,  3.4995e-01,  1.3400e-01, -4.9975e-01,\n",
              "        -2.7606e-01, -3.3675e-01, -1.4786e-01, -4.3405e-01,  1.6286e-01,\n",
              "        -2.0199e-01,  1.4544e-01, -4.1579e-01,  7.2142e-01,  5.0229e-02,\n",
              "         2.2854e-01,  3.2875e-01,  6.7382e-03, -4.0763e-01, -3.3011e-01,\n",
              "        -9.1815e-02,  3.6757e-01, -1.2671e-01,  3.2030e-01, -1.1395e-01,\n",
              "        -5.4180e-01, -3.1905e-01,  4.8444e-01, -3.1616e-01, -6.3157e-02,\n",
              "         7.1942e-01,  8.1258e-02, -2.1240e-02, -5.6368e-03, -1.7926e-01,\n",
              "        -3.2009e-01, -2.8840e-01,  7.7847e-01, -4.5017e-02, -3.0945e-01,\n",
              "        -1.7527e-01,  1.7451e-01,  2.5541e-01,  2.8751e-01,  4.8114e-01,\n",
              "         4.2397e-01, -5.8084e-01,  4.0212e-01,  2.0905e-01, -2.2190e-01,\n",
              "        -6.7272e-02, -1.6125e-01, -4.4701e-02, -7.9190e-02,  2.2595e-01,\n",
              "        -1.2307e-01, -1.7289e-01,  5.5469e-01, -8.1479e-02, -6.1574e-02,\n",
              "        -2.2017e-01,  1.3108e-01,  1.9292e-01,  1.5984e-01,  3.8795e-01,\n",
              "         1.1633e-01,  8.1654e-02,  8.7936e-02,  8.9818e-02,  1.8796e-01,\n",
              "         8.5572e-03,  3.0589e-01, -2.1046e-01,  9.4191e-02, -1.7978e-02,\n",
              "        -5.9961e-01, -2.4818e-01, -4.4255e-01,  8.8730e-02, -3.2792e-01,\n",
              "         2.6032e-01, -2.5244e-01,  2.1331e-01, -1.7616e-01, -4.2965e-01,\n",
              "         2.3022e-01, -6.6482e-01, -4.2774e-01, -1.6580e-01, -4.5382e-01,\n",
              "        -2.0307e-01, -5.2693e-01,  7.5872e-01, -8.3802e-02, -9.5648e-02,\n",
              "        -2.5832e-01,  1.5562e-01, -6.9494e-02,  1.2390e-01,  6.4311e-01,\n",
              "         6.3210e-01,  9.0335e-02,  2.4435e-01,  1.5503e-04, -9.3793e-02,\n",
              "        -7.4417e-02,  8.1851e-01, -7.6867e-03,  4.9987e-01,  1.7269e-01,\n",
              "        -6.7649e-02, -3.0403e-01, -4.3745e-01, -7.6301e-02, -1.9073e-01,\n",
              "         3.3495e-01, -4.7481e-01, -6.1074e-01, -2.6814e-01,  6.8875e-01,\n",
              "        -1.4930e-01,  5.0377e-01,  1.5243e-01, -3.8912e-01,  5.9175e-02,\n",
              "        -3.4562e-02,  3.6507e-01,  1.1397e-01,  3.6757e-02, -8.2319e-01,\n",
              "        -3.9829e-01,  5.4134e-01, -2.8465e-01, -8.0202e-01, -6.0656e-01,\n",
              "        -2.6521e-02, -5.6903e-02,  7.0342e-01, -5.3883e-01,  6.3126e-01,\n",
              "        -9.8318e-02, -2.8583e-01, -9.7073e-02, -3.8111e-01, -2.8166e-01,\n",
              "        -3.3377e-01, -1.5213e-02,  8.9935e-02, -2.3989e-01,  2.1049e-01,\n",
              "         6.0770e-03, -7.5477e-01, -1.3130e-01, -2.3706e-02, -1.5377e-01,\n",
              "        -3.9536e-02, -2.2700e-01, -1.6119e-01,  1.1412e-01,  9.7164e-02,\n",
              "         1.1511e-01,  2.2503e-01, -3.1961e-01, -2.5323e-01,  6.4890e-01,\n",
              "        -3.4573e-01, -2.1295e-01,  1.2878e-02])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga del modelo de Embedding BETO-Galén\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XC9dFB2pxYuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Auxiliary components\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "#from nlp_utils import *\n",
        "\n",
        "# BETO tokenizer\n",
        "!pip install keras_bert\n",
        "from keras_bert import load_vocabulary, Tokenizer\n",
        "model_path = \"drive/MyDrive/\"\n",
        "#https://github.com/dccuchile/beto/blob/master/config/uncased_2M/config.json\n",
        "config_path = model_path + \"config.json\"\n",
        "#https://github.com/guilopgar/ClinicalCodingTransformerES/tree/main/BETO/BETO-Galen\n",
        "checkpoint_path = \"drive/MyDrive/model.ckpt-1000000\"\n",
        "vocab_file = \"vocab.txt\"\n",
        "tokenizer = Tokenizer(token_dict=load_vocabulary(model_path + vocab_file), pad_index=1, cased=True)\n",
        "\n",
        "# Hyper-parameters\n",
        "#text_col = \"raw_text\"\n",
        "training = False\n",
        "trainable = True\n",
        "SEQ_LEN = 128\n",
        "#BATCH_SIZE = 16\n",
        "#EPOCHS = 28\n",
        "#LR = 3e-5\n",
        "#train_weight = 4\n",
        "#all_abs_weight = 1\n",
        "\n",
        "#random_seed = 0\n",
        "#tf.set_random_seed(random_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK30evoLxmvr",
        "outputId": "3268c650-f291-447c-b645-7dc9fa14a686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_bert\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras_bert) (1.21.6)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: keras_bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention\n",
            "  Building wheel for keras_bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras_bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33516 sha256=d9f37f69fd3bd4be864e2001c00626f15c6e68e8ead0820dc6f34d63f6c419d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/34/ed/6bbd71716d7bcea30d75e8bc5aeb94f4cb52636295c8343534\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12304 sha256=74f98d6bf73c8ef9ac528406ff76539d2e154c02406802251c2f9f695a9175f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/cd/a7/a8fa93f7e177eee0101fed63179f7a2fa3b53671ffaad82bfd\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3959 sha256=0161fe9e8420990b64cf26262b1491959913f198e19c3b22e9529d2728886940\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/bd/9a/ec6e575aaa50687d7af968bde7ce710b542eeaa9ee7978d4ba\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=abc4cfeb85cc4840d5055e0cd45edc844a89bf8b13583e563dfb1634844acc4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/2b/f4/28f4bab995fa99c26b761bc7f9aeb5bf6c81e9be6ccd0b853b\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=9a82a0bb5362ff41620f02beaea2ee27452b9af03118378da8658cf72fef685e\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/eb/bc/ce4bb467f5a7db6727f148f70bb0e52a62ef7edd41a19c8bdd\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6961 sha256=9e1e892f17edb67c69ef18f2d87ebde6f359891ebc18d2f29bc92c450f005894\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/c4/ff/7e13e4f102c3b7d73ff075a50fe3266f3ec2de898d5493a8a2\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=67ea76904b5d8cd4953fb0d49cbf9e8f2ce1d57d58d19ed702c189cf718b5341\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/15/39/59861ed531ef6c7c75810500eb22c68a425f82dde31d68630a\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18913 sha256=234d89db90f1964d96810e71847f0121ed7d2aed68a148bca8256c24a212a0d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/13/2d/3de7c76f618a8d162884ac5b726a8c2242ad88afa370f1e62f\n",
            "Successfully built keras_bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, keras-multi-head, keras-transformer, keras_bert\n",
            "Successfully installed keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 keras_bert-0.89.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from keras.backend.tensorflow_backend import set_session\n",
        "#from tensorflow.compat.v1.keras.backend import set_session\n",
        "\n",
        "# Prevent GPU memory allocation problems\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.allow_growth = True\n",
        "#set_session(tf.Session(config=config))"
      ],
      "metadata": {
        "id": "El70lF6XxZgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "model = load_trained_model_from_checkpoint(\n",
        "    config_file=config_path, \n",
        "    checkpoint_file=checkpoint_path, \n",
        "    training=training,                                       \n",
        "    trainable=trainable, \n",
        "    seq_len=SEQ_LEN\n",
        ")"
      ],
      "metadata": {
        "id": "Rf1rLlPjxefh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFPUv7KP1SHp",
        "outputId": "41ab2680-fb3b-4a24-94b3-6411d5a739b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'Input-Token')>,\n",
              " <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'Input-Segment')>]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://keras.io/api/layers/core_layers/input/\n",
        "\n",
        "shape: A shape tuple (integers), not including the batch size. For instance, shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors. Elements of this tuple can be None; 'None' elements represent dimensions where the shape is not known.\n",
        "\n",
        "https://www.kaggle.com/code/rajmehra03/a-detailed-explanation-of-keras-embedding-layer\n",
        "\n",
        "A Detailed Explanation of Keras Embedding Layer"
      ],
      "metadata": {
        "id": "ZRAF0QwNWRBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByuDs6jA4Y7L",
        "outputId": "1eb13ae3-db2c-43ee-f2c7-f9400fc4815a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'Encoder-12-FeedForward-Norm')>]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "bxWR8-owKjoq",
        "outputId": "f7e0a170-5c3f-41ee-9122-bcd1d07d55ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-46e15de059f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'eval'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using BETO tokenizer, each sentence is converted into a sequence of subwords, \n",
        "#which are further converted into vocabulary indices (input IDs) and segments arrays (BERT input tensors).\n",
        "\n",
        "tokenizer.encode('el paciente impaciente')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOrz229DQXKE",
        "outputId": "41366d96-06ea-4c10-e396-f9c878cd845f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4, 1039, 6420, 28601, 5], [0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize('el paciente impaciente')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnZBewF1QflM",
        "outputId": "c864bf99-a951-4185-ff9a-6fa281620b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'el', 'paciente', 'impaciente', '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'el paciente impaciente'"
      ],
      "metadata": {
        "id": "fQkM7kSwXBix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
        "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
        "\n",
        "def get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_ids(text, tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    #token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    token_ids = tokenizer.encode(text)[0]\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids"
      ],
      "metadata": {
        "id": "2M4vlLwUXiEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = get_ids(text, tokenizer.tokenize(text), tokenizer, SEQ_LEN)\n",
        "input_masks = get_masks(tokenizer.tokenize(text),SEQ_LEN)\n",
        "input_segments = get_segments(tokenizer.tokenize(text),SEQ_LEN)"
      ],
      "metadata": {
        "id": "vKgF8n1VWpPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_segments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXqK3nVAYWEW",
        "outputId": "e101c63a-e83e-4dc4-ec42-71f85ca24a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xGHFrXm5Wo6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
        "pool_embs, all_embs = model.predict([[tf.convert_to_tensor(input_ids),tf.convert_to_tensor(input_masks)],tf.convert_to_tensor(input_segments)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "OVIxMJasWoJO",
        "outputId": "21cacd36-5dc0-47e0-aac3-4d41f23bb520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-9777f8c48716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpool_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_segments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1791, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_1\" expects 2 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(32,) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(32,) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(32,) dtype=int32>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'El paciente presenta un abdomen distendido'"
      ],
      "metadata": {
        "id": "zRKkJChH6C-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "VhhCw6obRErB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJlu8uO9U0Hs",
        "outputId": "c6fa11b5-1f43-4638-cdab-2f21b8972a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4, 3, 30962, 6420, 4167, 1044, 24351, 9669, 30959, 1816, 5],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = ([1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1],[1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1.1])"
      ],
      "metadata": {
        "id": "kxrhkeVAW8TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokens_tensor = torch.tensor(encoded_text[0])\n",
        "#segments_tensor = torch.tensor(encoded_text[1])\n",
        "tokens_tensor = tf.convert_to_tensor(encoded_text[0])\n",
        "segments_tensor = tf.convert_to_tensor(encoded_text[1])"
      ],
      "metadata": {
        "id": "ztMp84BQRbXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjUHW5hsR9U-",
        "outputId": "211cbbbd-8a7b-4ae9-bdee-5221641124ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              "array([1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1,\n",
              "       1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1,\n",
              "       1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1,\n",
              "       1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1,\n",
              "       1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1,\n",
              "       1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1,\n",
              "       1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1,\n",
              "       1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1,\n",
              "       1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1,\n",
              "       1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#outputs = model(inputs=tf.convert_to_tensor(tokenizer.encode(text)))\n",
        "#outputs = model(inputs=[tokens_tensor, segments_tensor])\n",
        "outputs = model(inputs=[tokens_tensor,segments_tensor])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "eBXVX8NJQ5v9",
        "outputId": "dc35afd2-d585-4c9e-caf0-22128d981b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-182-d8384d532616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#outputs = model(inputs=tf.convert_to_tensor(tokenizer.encode(text)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#outputs = model(inputs=[tokens_tensor, segments_tensor])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegments_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7163\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7164\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"Embedding-Position\" (type PositionEmbedding).\n\nslice index 2 of dimension 0 out of bounds. [Op:StridedSlice] name: model_6/Embedding-Position/strided_slice/\n\nCall arguments received by layer \"Embedding-Position\" (type PositionEmbedding):\n  • inputs=tf.Tensor(shape=(128, 768), dtype=float32)\n  • kwargs={'mask': 'tf.Tensor(shape=(128,), dtype=bool)', 'training': 'False'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Add the special tokens.\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Split the sentence into tokens.\n",
        "#tokenized_text = tokenizer.tokenize(marked_text)\n",
        "tokenized_text = tokenizer.encode(marked_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indeces.\n",
        "# indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "indexed_tokens = tokenized_text.ids\n",
        "# Create inputs\n",
        "input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
        "token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n",
        "    tokenized_question.ids[1:]\n",
        ")\n",
        "\n",
        "# Mark each of the tokens as belonging to sentence \"1\".\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "b6LgQuIE5OYN",
        "outputId": "12a21e58-5c6d-4e91-8b2c-f1a3d75f84c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-b93906683c6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Map the token strings to their vocabulary indeces.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mindexed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# Create inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenized_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ids'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaTMa6nR6xNe",
        "outputId": "cbc2d52a-409b-4060-8e6a-4598a07f2619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  3449, 14397, 25099,  2556,  2050,  4895, 13878,  4487, 16173,\n",
            "          4305,  3527,   102]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(segments_tensors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvKiEHos68kI",
        "outputId": "615acbb4-a79f-4453-9490-1642328eecfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [tokens_tensor, segments_tensors]"
      ],
      "metadata": {
        "id": "R7LVo_vO7IYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb1ua4Z37WBz",
        "outputId": "43fb0cc1-ed9a-40c3-d775-df28ab63f76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  101,  3449, 14397, 25099,  2556,  2050,  4895, 13878,  4487, 16173,\n",
            "          4305,  3527,   102]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = SEQ_LEN\n",
        "\n",
        "input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "\n",
        "attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "    \n",
        "embeddings = model([input_ids,attention_masks])[1]\n",
        "    \n",
        "output = tf.keras.layers.Dense(3, activation=\"softmax\")(embeddings)"
      ],
      "metadata": {
        "id": "6Q2RcXnb9QWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outputs = model([tokens_tensor,segments_tensors])\n",
        "outputs = model(inputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "luUvwz6a7FAI",
        "outputId": "9a4150f6-6a9c-463f-f90a-9edc1cfd59a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-eae0dd7bc3cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#outputs = model([tokens_tensor,segments_tensors])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36mdisplay_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'as_list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the text through BERT, and collect all of the hidden states produced\n",
        "# from all 12 layers. \n",
        "with torch.no_grad():\n",
        "    print('antes de la magia')\n",
        "    outputs = model(tokens_tensor, segments_tensors)\n",
        "    print('después de la magia')\n",
        "\n",
        "    # Evaluating the model will return a different number of objects based on \n",
        "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
        "    # becase we set `output_hidden_states = True`, the third item will be the \n",
        "    # hidden states from all layers. See the documentation for more details:\n",
        "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "    hidden_states = outputs[2]\n",
        "\n",
        "# `hidden_states` has shape [13 x 1 x 22 x 768]\n",
        "\n",
        "# `token_vecs` is a tensor with shape [22 x 768]\n",
        "token_vecs = hidden_states[-2][0]\n",
        "\n",
        "# Calculate the average of all 22 token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)"
      ],
      "metadata": {
        "id": "R4wXwqwG6rn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embedding = NLP_get_BERT_embedding(model, text)"
      ],
      "metadata": {
        "id": "FKT5LQfdkYCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de las terminologías clínicas SNOMED-CT"
      ],
      "metadata": {
        "id": "luRQ4W8wpX_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def cargaProblemasSaludAHSpainExtension():\n",
        "  f = open('der2_Refset_ProblemasSaludAHSpainExtensionSnapshot_ES_20221201.txt')\n",
        "  terminos_snomedct_problemassaludAH = []\n",
        "  cont = 0\n",
        "  for row in f:\n",
        "    if cont>0 and cont<100:\n",
        "      values = row.strip().split(\"\\t\")\n",
        "      #id\teffectiveTime\tactive\tmoduleId\trefsetId\treferencedComponentId\tterm\n",
        "      #print(values)\n",
        "      termino = {}\n",
        "      termino[\"id\"] = values[0];\n",
        "      termino[\"effectiveTime\"] = values[1];\n",
        "      termino[\"active\"] = values[2];\n",
        "      termino[\"moduleId\"] = values[3];\n",
        "      termino[\"refsetId\"] = values[4];\n",
        "      termino[\"referencedComponentId\"] = values[5];\n",
        "      termino[\"term\"] = values[6];\n",
        "      termino[\"num_palabras\"] = len(values[6].split(\" \"));\n",
        "      #print(termino)\n",
        "      terminos_snomedct_problemassaludAH.append(termino)\n",
        "    cont = cont + 1\n",
        "\n",
        "  f.close();\n",
        "  return terminos_snomedct_problemassaludAH;"
      ],
      "metadata": {
        "id": "5sL1HVqZjlkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def anadirEmbedding(model,lista):\n",
        "  for item in lista:\n",
        "    doc = NLP_preprocesaDocumento(item[\"term\"])\n",
        "    print(doc)\n",
        "    item[\"embedding\"] = NLP_get_BERT_embedding(model, doc)\n"
      ],
      "metadata": {
        "id": "nBnZnJKcjydV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terminos_snomedct_problemassaludAH = cargaProblemasSaludAHSpainExtension();"
      ],
      "metadata": {
        "id": "OpciyOj9h5hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "terminos_snomedct_problemassaludAH = anadirEmbedding(model, terminos_snomedct_problemassaludAH);"
      ],
      "metadata": {
        "id": "b0WxjASUrcuY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "f1b16744-2e38-40fe-d557-7680cb0d0f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abdomar distendido\n",
            "aborto espontaneo\n",
            "aborto espontaneo completo\n",
            "aborto espontaneo incompleto\n",
            "abortir inadvertido\n",
            "aborto retenido\n",
            "abrasion\n",
            "abrasion corneal\n",
            "abrasion extremidad inferior\n",
            "absceso\n",
            "absceso abdominal\n",
            "absceso amigdalar\n",
            "absceso higado\n",
            "absceso glandulo Bartolino\n",
            "absceso mama\n",
            "absceso piel tejido celular subcutaneo\n",
            "absceso vulva\n",
            "absceso pie\n",
            "absceso pulmon\n",
            "absceso dental\n",
            "absceso periamigdalino\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-2deb4584f7d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mterminos_snomedct_problemassaludAH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manadirEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminos_snomedct_problemassaludAH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-da1e43a13114>\u001b[0m in \u001b[0;36manadirEmbedding\u001b[0;34m(model, lista)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manadirEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlista\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlista\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLP_preprocesaDocumento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"term\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLP_get_BERT_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-57369bf72190>\u001b[0m in \u001b[0;36mNLP_preprocesaDocumento\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mNLP_preprocesaDocumento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m#Cargamos el modelo en español\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es_core_news_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#Tokenizado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"blank:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \"\"\"\n\u001b[1;32m    467\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/es_core_news_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m     return load_model_from_path(\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     )\n\u001b[0;32m--> 514\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[1;32m   2138\u001b[0m             \u001b[0;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2140\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_link_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mdeserialize_vocab\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m   2114\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdeserialize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.from_disk\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/vectors.pyx\u001b[0m in \u001b[0;36mspacy.vectors.Vectors.from_disk\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m def from_disk(\n\u001b[0m\u001b[1;32m   1344\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[0mreaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for termino in terminos_snomedct_problemassaludAH:\n",
        "  print(\"Término: %s, preprocesado: %s\", (termino['term'], NLP_preprocesaDocumento(termino['term'])))"
      ],
      "metadata": {
        "id": "h1Y7rTuzrqYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtención de n-gramas del texto"
      ],
      "metadata": {
        "id": "9A4o1IJ7s9_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NLP_get_ngrams(text, n):\n",
        "    text = text.split()\n",
        "    output = []\n",
        "    for i in range(len(text) - n + 1):\n",
        "        output.append(text[i:i+n])\n",
        "    return output"
      ],
      "metadata": {
        "id": "LohpNnBFs-YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probando"
      ],
      "metadata": {
        "id": "0inAfeaKsDCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#informe = \"El paciente presenta un abdomen distendido, y no es capaz de volverse loco. El paciente presenta ansiedad y a veces un poco de apraxia.\"\n",
        "informe = \"Hay evidencia de hipoatenuación hepática difusa compatible con infiltración grasa . No hay dilatación de los conductos biliares intra o extrahepáticos. El paciente se encuentra en estado post colecistectomía. El bazo es normal. El páncreas es de contorno y características de atenuación normales. No hay evidencia de masa suprarrenal. Hay una hernia supraumbilical de tamaño moderado que contiene grasa. Los riñones son normales en tamaño, forma y configuración. No se identifican cálculos renales ni ureterales. No hay hidrouréter ni hidronefrosis. No hay evidencia de apendicitis. Hay varias asas de intestino delgado llenas de líquido, compatibles con una enteritis leve. No hay engrosamiento de la pared intestinal. No hay evidencia de obstrucción del intestino delgado o grueso. No hay evidencia de ascitis abdominal o linfadenopatía. No hay evidencia de masa vesical intrínseca o extrínseca . No hay ascitis pélvica ni linfadenopatía . El útero y los ovarios no presentan ninguna anomalía. Las imágenes de las bases pulmonares no muestran evidencia de masa pleural o parenquimatosa . No hay derrames pleurales. Hay cicatrices en el lóbulo medio derecho y en la língula, así como en ambas bases pulmonares. Las estructuras óseas están libres de lesiones líticas o blásticas. Se observan cambios degenerativos multinivel en la columna toracolumbar. Se observan calcificaciones dispersas en la aorta y en sus principales ramas, compatibles con la aterosclerosis.\"\n",
        "informe_preprocesado = NLP_preprocesaDocumento(informe)\n",
        "tokens = informe_preprocesado.split(\" \");\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdgQMdgQsDmT",
        "outputId": "671bd92c-3a6f-42fa-946b-a30e46a647d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['evidenciar', 'hipoatenuacion', 'hepatico', 'difusa', 'compatible', 'infiltracion', 'gra', 'dilatacion', 'conductir', 'biliar', 'intra', 'extrahepatico', 'paciente', 'post', 'colecistectomia', 'bazo', 'normal', 'pancrea', 'contorno', 'caracteristico', 'atenuacion', 'normal', 'evidenciar', 'masa', 'suprarrenal', 'hernia', 'supraumbilical', 'tamaño', 'moderado', 'contener', 'grasa', 'riñon', 'normal', 'tamaño', 'forma', 'configuracion', 'identificar', 'calculo', 'renal', 'ureteral', 'hidroureter', 'hidronefrosis', 'evidenciar', 'apendicitis', 'asa', 'intestino', 'delgado', 'lleno', 'liquido', 'compatible', 'enteriti', 'leve', 'engrosamiento', 'pared', 'intestinal', 'evidenciar', 'obstruccion', 'intestino', 'delgado', 'grueso', 'evidenciar', 'ascitis', 'abdominal', 'linfadenopatir', 'evidenciar', 'masa', 'vesical', 'intrinseca', 'extrinseco', 'ascitis', 'pelvico', 'linfadenopatir', 'utero', 'ovario', 'presentar', 'anomalia', 'imagen', 'bas', 'pulmonar', 'mostrar', 'evidencio', 'masa', 'pleural', 'parenquimato', 'derrame', 'pleural', 'cicatriz', 'lobulo', 'derecho', 'lingular', 'ambos', 'base', 'pulmonar', 'estructura', 'osea', 'libre', 'lesion', 'liticar', 'blastico', 'observar', 'cambio', 'degenerativo', 'multinivel', 'columno', 'toracolumbar', 'observar', 'calcificacion', 'disperso', 'aortir', 'principal', 'rama', 'compatible', 'aterosclerosis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#TEMPORAL, mientras no arregles de del preprocesado...\n",
        "for termino in terminos_snomedct_problemassaludAH:\n",
        "  termino[\"embedding\"] = NLP_get_BERT_embedding(model, termino[\"term\"])"
      ],
      "metadata": {
        "id": "wQK_-BI4v8qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from operator import itemgetter\n",
        "\n",
        "terminos_vinculados = []\n",
        "\n",
        "#Empezamos por ngramas más grandes a más pequeños\n",
        "for num_ngrams in range(6, 0, -1):\n",
        "  ngramas = NLP_get_ngrams(informe_preprocesado, num_ngrams)\n",
        "  for ngrama in ngramas:\n",
        "    doc = \" \".join([token for token in ngrama])\n",
        "    #print(doc)\n",
        "    doc_embedding = NLP_get_BERT_embedding(model, doc)\n",
        "    #print(doc_embedding.size())\n",
        "    vector_A = doc_embedding\n",
        "    #Por cada término de la terminología, comprobamos la similaridad con el ngrama previo\n",
        "    terminos_candidatos = []\n",
        "    for termino in terminos_snomedct_problemassaludAH:\n",
        "      vector_B = termino[\"embedding\"]\n",
        "\n",
        "      import torch\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "      cosine = cos(vector_A, vector_B)\n",
        "\n",
        "      #cosine = np.dot(vector_A,vector_B)/(norm(vector_A)*norm(vector_B))\n",
        "      if cosine > 0.9:\n",
        "        print(doc, \" \", termino[\"term\"], \" Cosine Similarity:\", cosine)\n",
        "        terminos_candidatos.append({\"term\": termino[\"term\"], \"embedding\": termino[\"embedding\"], \"cosine\": cosine})\n",
        "    #Nos quedamos con el término con mayor similaridad de los obtenidos\n",
        "    if terminos_candidatos:\n",
        "      terminos_candidatos_sorted = sorted(terminos_candidatos, key=itemgetter('cosine'), reverse=True) \n",
        "      terminos_vinculados.append(terminos_candidatos_sorted[0])\n",
        "\n",
        "#Eliminamos duplicados\n",
        "terminos_vinculados_sorted = sorted(terminos_vinculados, key=itemgetter('term')) \n",
        "termino_ant = {\"term\":\"\", \"cosine\": \"0.0\"}\n",
        "terminos_vinculados_deduplicated = []\n",
        "for termino in terminos_vinculados_sorted:\n",
        "  if termino[\"term\"] != termino_ant[\"term\"]:\n",
        "    if termino_ant[\"term\"] != \"\":\n",
        "      terminos_vinculados_deduplicated.append(termino_ant)\n",
        "    termino_ant = termino\n",
        "  else:\n",
        "    if termino_ant[\"cosine\"]<termino[\"cosine\"]:\n",
        "      termino_ant = termino\n",
        "terminos_vinculados_deduplicated.append(termino_ant)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYlwWo87tJKA",
        "outputId": "8bd80c66-7b18-4b67-8753-9d9f33cbf5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evidenciar hipoatenuacion hepatico difusa compatible infiltracion   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9011)\n",
            "hepatico difusa compatible infiltracion gra dilatacion   absceso perirrectal  Cosine Similarity: tensor(0.9030)\n",
            "hepatico difusa compatible infiltracion gra dilatacion   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9027)\n",
            "hepatico difusa compatible infiltracion gra dilatacion   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9055)\n",
            "compatible infiltracion gra dilatacion conductir biliar   absceso perirrectal  Cosine Similarity: tensor(0.9012)\n",
            "infiltracion gra dilatacion conductir biliar intra   absceso de la piel Y/O del tejido celular subcutáneo  Cosine Similarity: tensor(0.9025)\n",
            "contorno caracteristico atenuacion normal evidenciar masa   accidente causado por exposición a condiciones climáticas  Cosine Similarity: tensor(0.9033)\n",
            "supraumbilical tamaño moderado contener grasa riñon   absceso perirrectal  Cosine Similarity: tensor(0.9024)\n",
            "supraumbilical tamaño moderado contener grasa riñon   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9077)\n",
            "grasa riñon normal tamaño forma configuracion   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9039)\n",
            "tamaño forma configuracion identificar calculo renal   accidente por cuerpo extraño - orificio  Cosine Similarity: tensor(0.9012)\n",
            "lleno liquido compatible enteriti leve engrosamiento   absceso perirrectal  Cosine Similarity: tensor(0.9097)\n",
            "liquido compatible enteriti leve engrosamiento pared   absceso perirrectal  Cosine Similarity: tensor(0.9103)\n",
            "leve engrosamiento pared intestinal evidenciar obstruccion   adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9123)\n",
            "leve engrosamiento pared intestinal evidenciar obstruccion   afasia como efecto tardío de enfermedad cerebrovascular  Cosine Similarity: tensor(0.9062)\n",
            "engrosamiento pared intestinal evidenciar obstruccion intestino   adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9154)\n",
            "intestinal evidenciar obstruccion intestino delgado grueso   adenocarcinoma de intestino grueso  Cosine Similarity: tensor(0.9055)\n",
            "evidenciar masa vesical intrinseca extrinseco ascitis   absceso perirrectal  Cosine Similarity: tensor(0.9008)\n",
            "imagen bas pulmonar mostrar evidencio masa   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9085)\n",
            "pulmonar mostrar evidencio masa pleural parenquimato   absceso perirrectal  Cosine Similarity: tensor(0.9067)\n",
            "pulmonar mostrar evidencio masa pleural parenquimato   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9052)\n",
            "observar calcificacion disperso aortir principal rama   absceso perirrectal  Cosine Similarity: tensor(0.9015)\n",
            "evidenciar hipoatenuacion hepatico difusa compatible   abuso de cocaína sin adicción, episódico  Cosine Similarity: tensor(0.9019)\n",
            "evidenciar hipoatenuacion hepatico difusa compatible   accidente causado por exposición a condiciones climáticas  Cosine Similarity: tensor(0.9004)\n",
            "difusa compatible infiltracion gra dilatacion   abrasión de extremidad inferior  Cosine Similarity: tensor(0.9008)\n",
            "difusa compatible infiltracion gra dilatacion   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9042)\n",
            "difusa compatible infiltracion gra dilatacion   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9066)\n",
            "infiltracion gra dilatacion conductir biliar   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9145)\n",
            "infiltracion gra dilatacion conductir biliar   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9131)\n",
            "gra dilatacion conductir biliar intra   absceso perirrectal  Cosine Similarity: tensor(0.9182)\n",
            "atenuacion normal evidenciar masa suprarrenal   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9091)\n",
            "atenuacion normal evidenciar masa suprarrenal   accidente por cuerpo extraño - orificio  Cosine Similarity: tensor(0.9118)\n",
            "atenuacion normal evidenciar masa suprarrenal   adicción al tabaco, en remisión  Cosine Similarity: tensor(0.9062)\n",
            "supraumbilical tamaño moderado contener grasa   absceso perirrectal  Cosine Similarity: tensor(0.9048)\n",
            "supraumbilical tamaño moderado contener grasa   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9058)\n",
            "moderado contener grasa riñon normal   aborto retenido  Cosine Similarity: tensor(0.9002)\n",
            "moderado contener grasa riñon normal   accidente causado por maquinaria  Cosine Similarity: tensor(0.9016)\n",
            "moderado contener grasa riñon normal   accidente producido por filo de vidrio  Cosine Similarity: tensor(0.9026)\n",
            "moderado contener grasa riñon normal   acné con comedones  Cosine Similarity: tensor(0.9161)\n",
            "moderado contener grasa riñon normal   actualmente no fumador  Cosine Similarity: tensor(0.9021)\n",
            "evidenciar apendicitis asa intestino delgado   absceso de la piel Y/O del tejido celular subcutáneo  Cosine Similarity: tensor(0.9006)\n",
            "evidenciar apendicitis asa intestino delgado   abuso de cannabis sin adicción, episódico  Cosine Similarity: tensor(0.9005)\n",
            "evidenciar apendicitis asa intestino delgado   abuso de cocaína sin adicción, episódico  Cosine Similarity: tensor(0.9105)\n",
            "evidenciar apendicitis asa intestino delgado   accidente causado por maquinaria  Cosine Similarity: tensor(0.9120)\n",
            "evidenciar apendicitis asa intestino delgado   accidente producido por filo de vidrio  Cosine Similarity: tensor(0.9055)\n",
            "compatible enteriti leve engrosamiento pared   absceso perirrectal  Cosine Similarity: tensor(0.9143)\n",
            "leve engrosamiento pared intestinal evidenciar   adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9013)\n",
            "leve engrosamiento pared intestinal evidenciar   afasia como efecto tardío de enfermedad cerebrovascular  Cosine Similarity: tensor(0.9071)\n",
            "engrosamiento pared intestinal evidenciar obstruccion   adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9179)\n",
            "engrosamiento pared intestinal evidenciar obstruccion   afasia como efecto tardío de enfermedad cerebrovascular  Cosine Similarity: tensor(0.9000)\n",
            "evidenciar masa vesical intrinseca extrinseco   accidente causado por exposición a condiciones climáticas  Cosine Similarity: tensor(0.9040)\n",
            "pelvico linfadenopatir utero ovario presentar   absceso perirrectal  Cosine Similarity: tensor(0.9055)\n",
            "mostrar evidencio masa pleural parenquimato   absceso perirrectal  Cosine Similarity: tensor(0.9122)\n",
            "columno toracolumbar observar calcificacion disperso   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9015)\n",
            "evidenciar hipoatenuacion hepatico difusa   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9061)\n",
            "infiltracion gra dilatacion conductir   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9110)\n",
            "infiltracion gra dilatacion conductir   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9008)\n",
            "gra dilatacion conductir biliar   absceso perirrectal  Cosine Similarity: tensor(0.9122)\n",
            "gra dilatacion conductir biliar   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9036)\n",
            "gra dilatacion conductir biliar   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9063)\n",
            "dilatacion conductir biliar intra   absceso perirrectal  Cosine Similarity: tensor(0.9090)\n",
            "atenuacion normal evidenciar masa   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9264)\n",
            "atenuacion normal evidenciar masa   accidente por cuerpo extraño - orificio  Cosine Similarity: tensor(0.9044)\n",
            "atenuacion normal evidenciar masa   adicción al tabaco, en remisión  Cosine Similarity: tensor(0.9046)\n",
            "normal evidenciar masa suprarrenal   absceso de la piel Y/O del tejido celular subcutáneo  Cosine Similarity: tensor(0.9002)\n",
            "normal evidenciar masa suprarrenal   accidente causado por maquinaria  Cosine Similarity: tensor(0.9178)\n",
            "supraumbilical tamaño moderado contener   absceso perirrectal  Cosine Similarity: tensor(0.9043)\n",
            "supraumbilical tamaño moderado contener   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9028)\n",
            "tamaño moderado contener grasa   acné con comedones  Cosine Similarity: tensor(0.9016)\n",
            "moderado contener grasa riñon   accidente producido por filo de vidrio  Cosine Similarity: tensor(0.9054)\n",
            "moderado contener grasa riñon   acné con comedones  Cosine Similarity: tensor(0.9163)\n",
            "compatible enteriti leve engrosamiento   absceso perirrectal  Cosine Similarity: tensor(0.9176)\n",
            "enteriti leve engrosamiento pared   absceso perirrectal  Cosine Similarity: tensor(0.9059)\n",
            "engrosamiento pared intestinal evidenciar   adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9178)\n",
            "engrosamiento pared intestinal evidenciar   afasia como efecto tardío de enfermedad cerebrovascular  Cosine Similarity: tensor(0.9076)\n",
            "evidenciar obstruccion intestino delgado   adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9043)\n",
            "evidenciar masa vesical intrinseca   accidente causado por exposición a condiciones climáticas  Cosine Similarity: tensor(0.9059)\n",
            "linfadenopatir utero ovario presentar   absceso perirrectal  Cosine Similarity: tensor(0.9049)\n",
            "pulmonar mostrar evidencio masa   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9184)\n",
            "evidencio masa pleural parenquimato   absceso perirrectal  Cosine Similarity: tensor(0.9011)\n",
            "infiltracion gra dilatacion   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9203)\n",
            "gra dilatacion conductir   absceso perirrectal  Cosine Similarity: tensor(0.9006)\n",
            "dilatacion conductir biliar   absceso perirrectal  Cosine Similarity: tensor(0.9051)\n",
            "normal evidenciar masa   absceso de hígado  Cosine Similarity: tensor(0.9032)\n",
            "normal evidenciar masa   accidente causado por maquinaria  Cosine Similarity: tensor(0.9247)\n",
            "normal evidenciar masa   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9105)\n",
            "normal evidenciar masa   accidente producido por filo de vidrio  Cosine Similarity: tensor(0.9020)\n",
            "normal evidenciar masa   acné con comedones  Cosine Similarity: tensor(0.9013)\n",
            "normal evidenciar masa   actualmente no fumador  Cosine Similarity: tensor(0.9144)\n",
            "evidenciar masa suprarrenal   absceso de la piel Y/O del tejido celular subcutáneo  Cosine Similarity: tensor(0.9010)\n",
            "evidenciar masa suprarrenal   accidente causado por maquinaria  Cosine Similarity: tensor(0.9234)\n",
            "evidenciar masa suprarrenal   accidente por cuerpo extraño - orificio  Cosine Similarity: tensor(0.9049)\n",
            "evidenciar masa suprarrenal   accidente producido por filo de vidrio  Cosine Similarity: tensor(0.9061)\n",
            "evidenciar masa suprarrenal   adicción al tabaco, en remisión  Cosine Similarity: tensor(0.9073)\n",
            "tamaño moderado contener   acné con comedones  Cosine Similarity: tensor(0.9139)\n",
            "moderado contener grasa   aborto retenido  Cosine Similarity: tensor(0.9145)\n",
            "moderado contener grasa   accidente causado por maquinaria  Cosine Similarity: tensor(0.9067)\n",
            "moderado contener grasa   accidente producido por filo de vidrio  Cosine Similarity: tensor(0.9180)\n",
            "moderado contener grasa   acné con comedones  Cosine Similarity: tensor(0.9177)\n",
            "moderado contener grasa   actualmente no fumador  Cosine Similarity: tensor(0.9120)\n",
            "moderado contener grasa   adicción al tabaco, en remisión  Cosine Similarity: tensor(0.9071)\n",
            "evidenciar apendicitis asa   absceso perirrectal  Cosine Similarity: tensor(0.9016)\n",
            "enteriti leve engrosamiento   absceso perirrectal  Cosine Similarity: tensor(0.9128)\n",
            "leve engrosamiento pared   aborto retenido  Cosine Similarity: tensor(0.9230)\n",
            "leve engrosamiento pared   absceso de la piel Y/O del tejido celular subcutáneo  Cosine Similarity: tensor(0.9047)\n",
            "leve engrosamiento pared   accidente causado por maquinaria  Cosine Similarity: tensor(0.9141)\n",
            "leve engrosamiento pared   accidente producido por filo de vidrio  Cosine Similarity: tensor(0.9218)\n",
            "leve engrosamiento pared   acné con comedones  Cosine Similarity: tensor(0.9180)\n",
            "leve engrosamiento pared   actualmente no fumador  Cosine Similarity: tensor(0.9078)\n",
            "leve engrosamiento pared   adicción al tabaco, en remisión  Cosine Similarity: tensor(0.9108)\n",
            "leve engrosamiento pared   agresión por golpe con objeto romo u objeto arrojado  Cosine Similarity: tensor(0.9007)\n",
            "evidenciar obstruccion intestino   adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9053)\n",
            "evidenciar masa vesical   absceso de hígado  Cosine Similarity: tensor(0.9096)\n",
            "evidenciar masa vesical   accidente causado por exposición a condiciones climáticas  Cosine Similarity: tensor(0.9012)\n",
            "evidenciar masa vesical   accidente causado por maquinaria  Cosine Similarity: tensor(0.9226)\n",
            "evidenciar masa vesical   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9110)\n",
            "evidenciar masa vesical   accidente producido por filo de vidrio  Cosine Similarity: tensor(0.9078)\n",
            "evidenciar masa vesical   actualmente no fumador  Cosine Similarity: tensor(0.9108)\n",
            "evidenciar masa vesical   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9049)\n",
            "pulmonar mostrar evidencio   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9080)\n",
            "mostrar evidencio masa   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9018)\n",
            "observar calcificacion disperso   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9030)\n",
            "gra dilatacion   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9035)\n",
            "evidenciar masa   absceso de hígado  Cosine Similarity: tensor(0.9103)\n",
            "evidenciar masa   accidente causado por maquinaria  Cosine Similarity: tensor(0.9151)\n",
            "evidenciar masa   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9071)\n",
            "evidenciar masa   actualmente no fumador  Cosine Similarity: tensor(0.9137)\n",
            "evidenciar masa   adicción al tabaco, en remisión  Cosine Similarity: tensor(0.9084)\n",
            "moderado contener   aborto retenido  Cosine Similarity: tensor(0.9258)\n",
            "moderado contener   accidente producido por filo de vidrio  Cosine Similarity: tensor(0.9055)\n",
            "moderado contener   acné con comedones  Cosine Similarity: tensor(0.9238)\n",
            "moderado contener   actualmente no fumador  Cosine Similarity: tensor(0.9029)\n",
            "moderado contener   adicción al tabaco, en remisión  Cosine Similarity: tensor(0.9065)\n",
            "evidenciar apendicitis   absceso perirrectal  Cosine Similarity: tensor(0.9093)\n",
            "leve engrosamiento   aborto retenido  Cosine Similarity: tensor(0.9039)\n",
            "leve engrosamiento   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9029)\n",
            "engrosamiento pared   aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "engrosamiento pared   accidente causado por maquinaria  Cosine Similarity: tensor(0.9044)\n",
            "engrosamiento pared   accidente producido por filo de vidrio  Cosine Similarity: tensor(0.9042)\n",
            "engrosamiento pared   acné con comedones  Cosine Similarity: tensor(0.9071)\n",
            "engrosamiento pared   adicción al tabaco, en remisión  Cosine Similarity: tensor(0.9052)\n",
            "engrosamiento pared   agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9019)\n",
            "evidenciar ascitis   absceso de hígado  Cosine Similarity: tensor(0.9073)\n",
            "evidenciar masa   absceso de hígado  Cosine Similarity: tensor(0.9103)\n",
            "evidenciar masa   accidente causado por maquinaria  Cosine Similarity: tensor(0.9151)\n",
            "evidenciar masa   accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9071)\n",
            "evidenciar masa   actualmente no fumador  Cosine Similarity: tensor(0.9137)\n",
            "evidenciar masa   adicción al tabaco, en remisión  Cosine Similarity: tensor(0.9084)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for termino in terminos_vinculados_sorted:\n",
        "  print(termino[\"term\"], \" Cosine Similarity:\", termino[\"cosine\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mibi2kKjsJec",
        "outputId": "c8ccc392-ede6-4529-e2f4-ca0bac5a4fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aborto retenido  Cosine Similarity: tensor(0.9230)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9258)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9039)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "absceso de hígado  Cosine Similarity: tensor(0.9073)\n",
            "absceso de la piel Y/O del tejido celular subcutáneo  Cosine Similarity: tensor(0.9025)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9012)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9097)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9103)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9008)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9067)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9015)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9182)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9143)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9055)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9122)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9122)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9090)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9043)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9176)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9059)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9049)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9011)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9006)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9051)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9016)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9128)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9093)\n",
            "abuso de cocaína sin adicción, episódico  Cosine Similarity: tensor(0.9019)\n",
            "accidente causado por exposición a condiciones climáticas  Cosine Similarity: tensor(0.9033)\n",
            "accidente causado por exposición a condiciones climáticas  Cosine Similarity: tensor(0.9040)\n",
            "accidente causado por exposición a condiciones climáticas  Cosine Similarity: tensor(0.9059)\n",
            "accidente causado por maquinaria  Cosine Similarity: tensor(0.9120)\n",
            "accidente causado por maquinaria  Cosine Similarity: tensor(0.9178)\n",
            "accidente causado por maquinaria  Cosine Similarity: tensor(0.9247)\n",
            "accidente causado por maquinaria  Cosine Similarity: tensor(0.9234)\n",
            "accidente causado por maquinaria  Cosine Similarity: tensor(0.9226)\n",
            "accidente causado por maquinaria  Cosine Similarity: tensor(0.9151)\n",
            "accidente causado por maquinaria  Cosine Similarity: tensor(0.9151)\n",
            "accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9145)\n",
            "accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9110)\n",
            "accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9264)\n",
            "accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9203)\n",
            "accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9035)\n",
            "accidente por cuerpo extraño - orificio  Cosine Similarity: tensor(0.9012)\n",
            "accidente por cuerpo extraño - orificio  Cosine Similarity: tensor(0.9118)\n",
            "accidente producido por filo de vidrio  Cosine Similarity: tensor(0.9180)\n",
            "acné con comedones  Cosine Similarity: tensor(0.9161)\n",
            "acné con comedones  Cosine Similarity: tensor(0.9016)\n",
            "acné con comedones  Cosine Similarity: tensor(0.9163)\n",
            "acné con comedones  Cosine Similarity: tensor(0.9139)\n",
            "adenocarcinoma de intestino grueso  Cosine Similarity: tensor(0.9055)\n",
            "adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9123)\n",
            "adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9154)\n",
            "adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9179)\n",
            "adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9178)\n",
            "adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9043)\n",
            "adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9053)\n",
            "afasia como efecto tardío de enfermedad cerebrovascular  Cosine Similarity: tensor(0.9071)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9011)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9055)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9077)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9039)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9085)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9066)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9058)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9015)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9061)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9184)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9080)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9018)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9030)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for termino in terminos_vinculados_deduplicated:\n",
        "  print(termino[\"term\"], \" Cosine Similarity:\", termino[\"cosine\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxnLzoRsn65h",
        "outputId": "1517a3a7-6d00-4c8a-95fe-6c0ccb217993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "absceso de hígado  Cosine Similarity: tensor(0.9073)\n",
            "absceso de la piel Y/O del tejido celular subcutáneo  Cosine Similarity: tensor(0.9025)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9182)\n",
            "abuso de cocaína sin adicción, episódico  Cosine Similarity: tensor(0.9019)\n",
            "accidente causado por exposición a condiciones climáticas  Cosine Similarity: tensor(0.9059)\n",
            "accidente causado por maquinaria  Cosine Similarity: tensor(0.9247)\n",
            "accidente durante una actividad doméstica  Cosine Similarity: tensor(0.9264)\n",
            "accidente por cuerpo extraño - orificio  Cosine Similarity: tensor(0.9118)\n",
            "accidente producido por filo de vidrio  Cosine Similarity: tensor(0.9180)\n",
            "acné con comedones  Cosine Similarity: tensor(0.9163)\n",
            "adenocarcinoma de intestino grueso  Cosine Similarity: tensor(0.9055)\n",
            "adherencias intestinales con obstrucción  Cosine Similarity: tensor(0.9179)\n",
            "afasia como efecto tardío de enfermedad cerebrovascular  Cosine Similarity: tensor(0.9071)\n",
            "agujero retiniano redondo sin desprendimiento  Cosine Similarity: tensor(0.9184)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Nos quedamos sólo con los términos más parecidos de entre los similares (EXPERIMENTAL)\n",
        "\n",
        "#stu_details = [[\"Aisha\",30], [\"Bhavs\",40],[\"Cat\", 35],[\"Sam\",40],[\"Andre\",35],[\"Trina\",40],[\"Robbie\",30],[\"Beck\",35]]\n",
        "all_values = [dictionary[\"term\"] for dictionary in terminos_vinculados_deduplicated]\n",
        "unique_values = set(all_values)\n",
        "print(unique_values)\n",
        "result = []\n",
        "for value in unique_values:\n",
        "  this_group = []\n",
        "  for dictionary in terminos_vinculados_deduplicated:\n",
        "    if dictionary[\"term\"] == value:\n",
        "      this_group.append(dictionary[\"term\"])\n",
        "  result.append(this_group)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdEbntl-gI2t",
        "outputId": "32db807e-a162-4af9-f33c-ecd5324f1d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'adenocarcinoma de intestino grueso', 'afasia como efecto tardío de enfermedad cerebrovascular', 'accidente por cuerpo extraño - orificio', 'adherencias intestinales con obstrucción', 'absceso perirrectal', 'accidente causado por exposición a condiciones climáticas', 'absceso de hígado', 'aborto retenido', 'absceso de la piel Y/O del tejido celular subcutáneo', 'accidente causado por maquinaria', 'agujero retiniano redondo sin desprendimiento', 'accidente durante una actividad doméstica', 'abuso de cocaína sin adicción, episódico', 'accidente producido por filo de vidrio', 'acné con comedones'}\n",
            "[['adenocarcinoma de intestino grueso'], ['afasia como efecto tardío de enfermedad cerebrovascular'], ['accidente por cuerpo extraño - orificio'], ['adherencias intestinales con obstrucción'], ['absceso perirrectal'], ['accidente causado por exposición a condiciones climáticas'], ['absceso de hígado'], ['aborto retenido'], ['absceso de la piel Y/O del tejido celular subcutáneo'], ['accidente causado por maquinaria'], ['agujero retiniano redondo sin desprendimiento'], ['accidente durante una actividad doméstica'], ['abuso de cocaína sin adicción, episódico'], ['accidente producido por filo de vidrio'], ['acné con comedones']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Nos quedamos sólo con los términos más parecidos de entre los similares (EXPERIMENTAL - A AVANZAR)\n",
        "terminos_resultado = []\n",
        "for termino in terminos_vinculados_deduplicated:\n",
        "  vector_A = termino[\"embedding\"]\n",
        "  termino_candidato = termino\n",
        "  for termino2 in terminos_vinculados_deduplicated:\n",
        "    if termino[\"term\"] != termino2[\"term\"]:\n",
        "      vector_B = termino2[\"embedding\"]\n",
        "      \n",
        "      import torch\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "      cosine = cos(vector_A, vector_B)\n",
        "      \n",
        "      #cosine = np.dot(vector_A,vector_B)/(norm(vector_A)*norm(vector_B))\n",
        "      #Si los términos son similares\n",
        "      if cosine > 0.70:\n",
        "        #print(termino[\"term\"], \" \", termino2[\"term\"], \" Cosine Similarity:\", cosine)\n",
        "        if termino_candidato[\"cosine\"]<termino2[\"cosine\"]:\n",
        "          termino_candidato=termino2\n",
        "  terminos_resultado.append(termino_candidato)\n",
        "  print(termino_candidato[\"term\"], \" Cosine Similarity:\", termino_candidato[\"cosine\"])\n",
        "        #terminos_candidatos.append({\"term\": termino[\"term\"], \"embedding\": termino[\"embedding\"], \"cosine\": cosine})\n",
        "      \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGj5FeM9x7EV",
        "outputId": "f9b8c1f5-714b-47ba-b4d7-6183afef1cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9182)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Eliminamos duplicados\n",
        "terminos_resultado_sorted = sorted(terminos_resultado, key=itemgetter('term')) \n",
        "termino_ant = {\"term\":\"\", \"cosine\": \"0.0\"}\n",
        "terminos_resultado_deduplicated = []\n",
        "for termino in terminos_resultado_sorted:\n",
        "  if termino[\"term\"] != termino_ant[\"term\"]:\n",
        "    if termino_ant[\"term\"] != \"\":\n",
        "      terminos_resultado_deduplicated.append(termino_ant)\n",
        "      print(termino_ant[\"term\"], \" Cosine Similarity:\", termino_ant[\"cosine\"])\n",
        "    termino_ant = termino\n",
        "  else:\n",
        "    if termino_ant[\"cosine\"]<termino[\"cosine\"]:\n",
        "      termino_ant = termino\n",
        "terminos_resultado_deduplicated.append(termino_ant)\n",
        "print(termino_ant[\"term\"], \" Cosine Similarity:\", termino_ant[\"cosine\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql2iIybSnfgg",
        "outputId": "522e3479-17eb-4175-acb5-844a41c2fa24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aborto retenido  Cosine Similarity: tensor(0.9319)\n",
            "absceso perirrectal  Cosine Similarity: tensor(0.9182)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens:\n",
        "  #vector_A = model.wv.get_vector(token)\n",
        "  vector_A = NLP_get_BERT_embedding(model, token)\n",
        "  for termino in terminos_snomedct_problemassaludAH:\n",
        "    vector_B = termino[\"embedding\"]\n",
        "    import torch\n",
        "    cos = torch.nn.CosineSimilarity(dim=0)\n",
        "    cosine = cos(vector_A, vector_B)\n",
        "    #cosine = np.dot(vector_A,vector_B)/(norm(vector_A)*norm(vector_B))\n",
        "    if cosine > 0.90:\n",
        "      print(token, \" \", termino[\"term\"], \" Cosine Similarity:\", cosine)"
      ],
      "metadata": {
        "id": "4CagAkddskVD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "48a8e941-3d44-4d8b-ee2c-79f675dbc8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-28c3be0b80d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m#vector_A = model.wv.get_vector(token)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mvector_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLP_get_BERT_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtermino\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterminos_snomedct_problemassaludAH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvector_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtermino\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-833f2d6452b0>\u001b[0m in \u001b[0;36mNLP_get_BERT_embedding\u001b[0;34m(model, text)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0;31m# Evaluating the model will return a different number of objects based on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1021\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1022\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 )\n\u001b[1;32m    609\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    611\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 426\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}